{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox, Layout,VBox\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in Disaster Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12832\\2711977763.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  disaster_df['designatedIncidentTypes'].fillna(disaster_df['incidentType'], inplace = True)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12832\\2711977763.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  disaster_df['designatedIncidentTypes'].fillna(disaster_df['incidentType'], inplace = True)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12832\\2711977763.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  disaster_df.drop_duplicates('femaDeclarationString', inplace = True)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12832\\2711977763.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  disaster_merge_df['state'].replace(to_replace=state_dict, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# DISASTER DATASET\n",
    "disaster_data_link = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/DisasterDeclarationsSummaries.csv'\n",
    "\n",
    "#create dataframe from FEMA disaster declarations summaries csv\n",
    "file = pd.read_csv(disaster_data_link, low_memory=False)\n",
    "df = pd.DataFrame(file)\n",
    "\n",
    "#dictionary to convert state/territory designators to full word strings\n",
    "state_dict = {'AL':'Alabama','AK':'Alaska','AZ':'Arizona','AR':'Arkansas','CA':'California','CO':'Colorado','CT':'Connecticut',\n",
    "             'DE':'Delaware', 'FL':'Florida','GA':'Georgia','HI':'Hawaii','ID':'Idaho','IL':'Illinois','IN':'Indiana','IA':'Iowa',\n",
    "             'KS':'Kansas','KY':'Kentucky','LA':'Louisiana','ME':'Maine','MD':'Maryland','MA':'Massachusetts','MI':'Michigan',\n",
    "             'MN':'Minnesota','MS':'Mississippi','MO':'Missouri','MT':'Montana','NE':'Nebraska','NV':'Nevada','NH':'New Hampshire',\n",
    "             'NM':'New Mexico','NY':'New York','NJ':'New Jersey','NC':'North Carolina','ND':'North Dakota','OH':'Ohio',\n",
    "             'OK':'Oklahoma','OR':'Oregon','PA':'Pennsylvania','RI':'Rhode Island','SC':'South Carolina','SD':'South Dakota',\n",
    "              'TN':'Tennessee','TX':'Texas','UT':'Utah','VT':'Vermont','VA':'Virginia','WA':'Washington','WV':'West Virginia',\n",
    "             'WI':'Wisconsin','WY':'Wyoming','DC':'Washington, DC','GU':'Guam','PR':'Puerto Rico','AS':'American Samoa',\n",
    "             'MP':'Northern Mariana Islands','FM':'Federated States of Micronesia','MH':'Marshall Islands','PW':'Palau'}\n",
    "\n",
    "#create a list of the 48 contiguous states.  Alaska, Hawaii, DC, and territories are omitted\n",
    "state_list = ['AL','AZ','AR','CA','CO','CT','DE', 'FL','GA','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA','MI',\n",
    "             'MN','MS','MO','MT','NE','NV','NH','NM','NY','NJ','NC','ND','OH','OK','OR','PA','RI','SC','SD',\n",
    "              'TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "\n",
    "#set using only natural disasters that could be potentially caused by climate change\n",
    "natural_disaster = ['Fire','Flood','Severe Storm','Straight-Line Winds','Winter Storm','Hurricane','Tornado','Tropical Storm',\n",
    "                   'Mud/Landslide','Snowstorm','Coastal Storm','Severe Ice Storm','Typhoon','Freezing','Drought','Fishing Losses',\n",
    "                   'Other', 'Dam/Levee Break','Tropical Depression']\n",
    "\n",
    "#manmade or other disasters that would not be caused by climate change\n",
    "nonweather_disaster = ['Earthquake','Other','Biological','Dam/Levee Break','Volcanic Eruption','Toxic Substances','Chemical',\n",
    "                      'Terrorist','Human Cause','Tsunami','Civil Unrest','Nuclear','Explosion','Tidal Wave']\n",
    "\n",
    "#dictionary to convert disaster codes to strings representing each type of disaster\n",
    "disaster_dict = {'0':'Not applicable','1':'Explosion','2':'Straight-Line Winds','3':'Tidal Wave','4':'Tropical Storm',\n",
    "                '5':'Winter Storm','A':'Tsunami','B':'Biological','C':'Coastal Storm','D':'Drought','E':'Earthquake',\n",
    "                 'F':'Flood','G':'Freezing','H':'Hurricane','I':'Terrorist','J':'Typhoon','K':'Dam/Levee Break','L':'Chemical',\n",
    "                'M':'Mud/Landslide','N':'Nuclear','O':'Severe Ice Storm','P':'Fishing Losses','Q':'Crop Losses','R':'Fire',\n",
    "                'S':'Snowstorm','T':'Tornado','U':'Civil Unrest', 'V':'Volcanic Eruption','W':'Severe Storm','X':'Toxic Substances',\n",
    "                'Y':'Human Cause','Z':'Other', '8':'Tropical Depression'}\n",
    "\n",
    "\n",
    "                #select columns necessary for data analysis, add empty columns for each natural disaster type\n",
    "column_list = ['femaDeclarationString','state','incidentType','incidentBeginDate','fipsStateCode','region',\n",
    "               'designatedIncidentTypes','declarationTitle']+natural_disaster\n",
    "df = df.reindex(columns=column_list, fill_value=0)\n",
    "\n",
    "\n",
    "#convert strings to datetime objects\n",
    "df['incidentBeginDate']=pd.to_datetime(df['incidentBeginDate'])\n",
    "\n",
    "\n",
    "#create columns for year, month, and day for later analysis\n",
    "df['year'] = df['incidentBeginDate'].dt.year\n",
    "df['month'] = df['incidentBeginDate'].dt.month\n",
    "df['day'] = df['incidentBeginDate'].dt.day\n",
    "\n",
    "\n",
    "#select only disasters where primary incident type is in the natural disaster list, only states in CONUS, and only\n",
    "#in range 1953-2023\n",
    "disaster_df = df[(df['incidentType'].isin(natural_disaster)) & (df['state'].isin(state_list)) \n",
    "& (df['year'].isin(range(1953,2024)))]\n",
    "\n",
    "disaster_df.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "#fill in the missing values in the designatedIncidentTypes column using the indicidentType column\n",
    "disaster_df['designatedIncidentTypes'].fillna(disaster_df['incidentType'], inplace = True)\n",
    "\n",
    "\n",
    "#this block uses the incident codes in disaster_dict to populate the respective incident columns in disaster_df\n",
    "for i in range(0,len(disaster_df)):\n",
    "    incident = disaster_df.loc[i,'designatedIncidentTypes']\n",
    "    incident_list = incident.split(',')\n",
    "    \n",
    "    #if there is only one incident type, add 1 to the corresponding column\n",
    "    if ((len(incident_list) == 1) & (len(incident_list[0])>1)):\n",
    "        col = incident_list[0]\n",
    "        try:\n",
    "            disaster_df.loc[i, col] += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    #if there are multiple incident types, add 1 to the corresponding column for each\n",
    "    else:\n",
    "        #create a new list, this combines the incident list from designatedIncidentTypes and incidentType\n",
    "        new_list = []\n",
    "        for inc in incident_list:\n",
    "            #add longform name to new_list\n",
    "            if len(inc) == 1:\n",
    "                new_list.append(disaster_dict[inc])\n",
    "        #add incidentType to new_list if not already in new_list        \n",
    "        if disaster_df.loc[i, 'incidentType'] not in new_list:\n",
    "            new_list.append(disaster_df.loc[i, 'incidentType'])\n",
    "        #ensures that code skips over non-natural disasters        \n",
    "        for val in new_list:\n",
    "            try:\n",
    "                disaster_df.loc[i, val] += 1\n",
    "            except: \n",
    "                continue\n",
    "\n",
    "\n",
    "#drop duplicate disaster declarations, this eliminates duplicate county declarations in each state\n",
    "disaster_df.drop_duplicates('femaDeclarationString', inplace = True)\n",
    "disaster_merge_df = disaster_df.drop(columns=['index', 'incidentType','femaDeclarationString','incidentBeginDate','designatedIncidentTypes','declarationTitle','day'])\n",
    "disaster_merge_df['state'].replace(to_replace=state_dict, inplace=True)\n",
    "\n",
    "\n",
    "#create a dataframe from groupby that totals the disasters for each month by state.  \n",
    "#this is for state-level analysis only.  do not use for region or national level analysis.\n",
    "disaster_group = disaster_merge_df.groupby(by=['year','month','state']).agg(fire = ('Fire','sum'),\n",
    "                                                                 flood = ('Flood','sum'),\n",
    "                                                                  severe = ('Severe Storm','sum'),\n",
    "                                                                  straight_line_winds= ('Straight-Line Winds','sum'),\n",
    "                                                                  winter_storm = ('Winter Storm','sum'),\n",
    "                                                                  hurricane = ('Hurricane','sum'),\n",
    "                                                                  tornado = ('Tornado','sum'),\n",
    "                                                                  tropical_storm = ('Tropical Storm','sum'),\n",
    "                                                                  landslide = ('Mud/Landslide','sum'),\n",
    "                                                                  snowstorm = ('Snowstorm','sum'),\n",
    "                                                                  coastal_storm = ('Coastal Storm','sum'),\n",
    "                                                                  ice_storm = ('Severe Ice Storm','sum'),\n",
    "                                                                  typhoon = ('Typhoon','sum'),\n",
    "                                                                  freezing = ('Freezing','sum'),\n",
    "                                                                  drought = ('Drought','sum'),\n",
    "                                                                  fishing_loss = ('Fishing Losses','sum'),\n",
    "                                                                   other = ('Other','sum'), \n",
    "                                                                  dam_break = ('Dam/Levee Break','sum'),\n",
    "                                                                  tropical_depression = ('Tropical Depression','sum')\n",
    "                                                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# revised disaster_group dataframe addressing the issue about region code disappering from previous grouping \\n# this approach should return the same data \\ndisaster_group = disaster_merge_df.groupby(['year','month','state', 'region']).sum().drop('fipsStateCode', axis=1)\\ndisaster_group = disaster_group.reset_index().groupby(['year','month','state']).sum()\\ndisaster_group['region'] = disaster_group['region'].astype(str)\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# revised disaster_group dataframe addressing the issue about region code disappering from previous grouping \n",
    "# this approach should return the same data \n",
    "disaster_group = disaster_merge_df.groupby(['year','month','state', 'region']).sum().drop('fipsStateCode', axis=1)\n",
    "disaster_group = disaster_group.reset_index().groupby(['year','month','state']).sum()\n",
    "disaster_group['region'] = disaster_group['region'].astype(str)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Weather Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEATHER DATASET\n",
    "\n",
    "# collections of data URLs \n",
    "\n",
    "# Direct Link to NCEI Website, currently down as of 09/30/24\n",
    "\"\"\"\n",
    "data_url_percipitation = 'https://www.ncei.noaa.gov/pub/data/cirs/climdiv/climdiv-pcpnst-v1.0.0-20240906'\n",
    "data_url_max_temp = 'https://www.ncei.noaa.gov/pub/data/cirs/climdiv/climdiv-tmaxst-v1.0.0-20240906'\n",
    "data_url_min_temp = 'https://www.ncei.noaa.gov/pub/data/cirs/climdiv/climdiv-tminst-v1.0.0-20240906'\n",
    "data_url_avg_temp = 'https://www.ncei.noaa.gov/pub/data/cirs/climdiv/climdiv-tmpcst-v1.0.0-20240906'\n",
    "data_url_phdi = 'https://www.ncei.noaa.gov/pub/data/cirs/climdiv/climdiv-phdist-v1.0.0-20240906'\n",
    "data_url_pdmi = 'https://www.ncei.noaa.gov/pub/data/cirs/climdiv/climdiv-pmdist-v1.0.0-20240906'\n",
    "data_url_pdsi = 'https://www.ncei.noaa.gddov/pub/data/cirs/climdiv/climdiv-pdsist-v1.0.0-20240906'\n",
    "\"\"\"\n",
    "\n",
    "# Backup Dataset in Github \n",
    "data_url_percipitation = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/weather_data/climdiv-pcpnst'\n",
    "data_url_max_temp = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/weather_data/climdiv-tmaxst'\n",
    "data_url_min_temp = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/weather_data/climdiv-tminst'\n",
    "data_url_avg_temp = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/weather_data/climdiv-tmpcst'\n",
    "data_url_phdi = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/weather_data/climdiv-phdist'\n",
    "data_url_pdmi = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/weather_data/climdiv-pmdist'\n",
    "data_url_pdsi = 'https://raw.githubusercontent.com/xquinnma/milestone/refs/heads/main/weather_data/climdiv-pdsist'\n",
    "\n",
    "\n",
    "# column names for the data URLs - Month 1 to 12\n",
    "data_column_names = ['id', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "\n",
    "# dictionary for state code and state names \n",
    "state_code_dict = {\n",
    "    '1':'Alabama', '2':'Arizona', '3':'Arkansas', '4':'California', '5':'Colorado',\n",
    "    '6':'Connecticut', '7':'Delaware', '8':'Florida', '9':'Georgia', '10':'Idaho',\n",
    "    '11':'Illinois', '12':'Indiana', '13':'Iowa', '14':'Kansas', '15':'Kentucky',\n",
    "    '16':'Louisiana', '17':'Maine', '18':'Maryland', '19':'Massachusetts', '20':'Michigan',\n",
    "    '21':'Minnesota', '22':'Mississippi', '23':'Missouri', '24':'Montana', '25':'Nebraska',\n",
    "    '26':'Nevada', '27':'New Hampshire', '28':'New Jersey', '29':'New Mexico', '30':'New York',\n",
    "    '31':'North Carolina', '32':'North Dakota', '33':'Ohio', '34':'Oklahoma', '35':'Oregon',\n",
    "    '36':'Pennsylvania', '37':'Rhode Island', '38':'South Carolina', '39':'South Dakota', '40':'Tennessee',\n",
    "    '41':'Texas', '42':'Utah', '43':'Vermont', '44':'Virginia', '45':'Washington',\n",
    "    '46':'West Virginia', '47':'Wisconsin', '48':'Wyoming'\n",
    "                   }\n",
    "\n",
    "\n",
    "\n",
    "#fucntion that cleans data\n",
    "\n",
    "def format_df(input_url, feature_name, start_year=1953, end_year=2023):\n",
    "    df = pd.read_csv(input_url, header=None, names=data_column_names, delimiter=r\"\\s+\", dtype= str)\n",
    "    df = df.set_index('id').stack().reset_index().rename(columns={'level_1':'month', 0:feature_name})\n",
    "\n",
    "    # create columns for state code and year from id column\n",
    "    # convert year and month to datetime data\n",
    "    df['state_code'] = df['id'].astype(str).str[:3].astype(int)\n",
    "    df['year'] = pd.to_datetime(df['id'].astype(str).str[-4:]).dt.year\n",
    "    df['month'] = pd.to_datetime(df['month'], format='%m').dt.month\n",
    "    \n",
    "    # limit state code < 48 is continental US\n",
    "    df = df[df['state_code'] < 49]\n",
    "    # limit data between year 1953 and 2023\n",
    "    df = df[(df['year'] >= start_year) & (df['year'] <= end_year)]\n",
    "\n",
    "    # convert state code to state name \n",
    "    df['state'] = df['state_code'].astype(str).map(state_code_dict)\n",
    "\n",
    "    # dropping unnecessary coloumns and shuffle column order and reset index\n",
    "    df = df.drop(['id', 'state_code'], axis=1)\n",
    "    \n",
    "    # convert column to floats\n",
    "    df = df.astype({feature_name : 'float'})\n",
    "    \n",
    "    # reorder columns\n",
    "    df = df[['year', 'month', 'state', feature_name]].reset_index(drop=True)\n",
    "    \n",
    "    #print(df.dtypes)\n",
    "\n",
    "    return df.copy()\n",
    "\n",
    "\n",
    "\n",
    "# creating individual dataframes for each feature\n",
    "df_percipitation = format_df(data_url_percipitation, 'percipitation')\n",
    "df_max_temp =   format_df(data_url_max_temp, 'max_temp')\n",
    "df_min_temp = format_df(data_url_min_temp, 'min_temp')\n",
    "df_avg_temp = format_df(data_url_avg_temp, 'avg_temp')\n",
    "df_phdi = format_df(data_url_phdi, 'pdhi')\n",
    "df_phmi = format_df(data_url_pdmi, 'pdmi')\n",
    "\n",
    "# combine all data \n",
    "df_all_list = [df_percipitation, df_max_temp, df_min_temp, df_avg_temp, df_phdi, df_phmi]\n",
    "\n",
    "df_combined = df_all_list[0].copy()\n",
    "\n",
    "for i in range(len(df_all_list)):\n",
    "    if i!=0:\n",
    "        df_combined = pd.merge(df_combined, df_all_list[i], on=['year', 'month', 'state'])\n",
    "\n",
    "\n",
    "df_combined_grouped = df_combined.groupby(['year', 'month', 'state']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>fire</th>\n",
       "      <th>flood</th>\n",
       "      <th>severe</th>\n",
       "      <th>straight_line_winds</th>\n",
       "      <th>winter_storm</th>\n",
       "      <th>hurricane</th>\n",
       "      <th>tornado</th>\n",
       "      <th>tropical_storm</th>\n",
       "      <th>landslide</th>\n",
       "      <th>...</th>\n",
       "      <th>fishing_loss</th>\n",
       "      <th>other</th>\n",
       "      <th>dam_break</th>\n",
       "      <th>tropical_depression</th>\n",
       "      <th>percipitation</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>pdhi</th>\n",
       "      <th>pdmi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1953</th>\n",
       "      <th>Connecticut</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.39</td>\n",
       "      <td>733.0</td>\n",
       "      <td>479.5</td>\n",
       "      <td>606.3</td>\n",
       "      <td>19.68</td>\n",
       "      <td>13.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.55</td>\n",
       "      <td>645.6</td>\n",
       "      <td>387.8</td>\n",
       "      <td>516.8</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.64</td>\n",
       "      <td>722.6</td>\n",
       "      <td>468.1</td>\n",
       "      <td>595.3</td>\n",
       "      <td>9.80</td>\n",
       "      <td>8.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.38</td>\n",
       "      <td>680.6</td>\n",
       "      <td>404.5</td>\n",
       "      <td>542.7</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.83</td>\n",
       "      <td>728.2</td>\n",
       "      <td>498.8</td>\n",
       "      <td>613.5</td>\n",
       "      <td>16.93</td>\n",
       "      <td>11.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           month  fire  flood  severe  straight_line_winds  \\\n",
       "region year state                                                            \n",
       "1      1953 Connecticut       78   0.0    0.0     0.0                  0.0   \n",
       "            Maine             78   0.0    0.0     0.0                  0.0   \n",
       "            Massachusetts     78   0.0    0.0     0.0                  0.0   \n",
       "            New Hampshire     78   1.0    0.0     0.0                  0.0   \n",
       "            Rhode Island      78   0.0    0.0     0.0                  0.0   \n",
       "\n",
       "                           winter_storm  hurricane  tornado  tropical_storm  \\\n",
       "region year state                                                             \n",
       "1      1953 Connecticut             0.0        0.0      0.0             0.0   \n",
       "            Maine                   0.0        0.0      0.0             0.0   \n",
       "            Massachusetts           0.0        0.0      1.0             0.0   \n",
       "            New Hampshire           0.0        0.0      0.0             0.0   \n",
       "            Rhode Island            0.0        0.0      0.0             0.0   \n",
       "\n",
       "                           landslide  ...  fishing_loss  other  dam_break  \\\n",
       "region year state                     ...                                   \n",
       "1      1953 Connecticut          0.0  ...           0.0    0.0        0.0   \n",
       "            Maine                0.0  ...           0.0    0.0        0.0   \n",
       "            Massachusetts        0.0  ...           0.0    0.0        0.0   \n",
       "            New Hampshire        0.0  ...           0.0    0.0        0.0   \n",
       "            Rhode Island         0.0  ...           0.0    0.0        0.0   \n",
       "\n",
       "                           tropical_depression  percipitation  max_temp  \\\n",
       "region year state                                                         \n",
       "1      1953 Connecticut                    0.0          55.39     733.0   \n",
       "            Maine                          0.0          46.55     645.6   \n",
       "            Massachusetts                  0.0          53.64     722.6   \n",
       "            New Hampshire                  0.0          47.38     680.6   \n",
       "            Rhode Island                   0.0          56.83     728.2   \n",
       "\n",
       "                           min_temp  avg_temp   pdhi   pdmi  \n",
       "region year state                                            \n",
       "1      1953 Connecticut       479.5     606.3  19.68  13.69  \n",
       "            Maine             387.8     516.8   3.11   1.56  \n",
       "            Massachusetts     468.1     595.3   9.80   8.15  \n",
       "            New Hampshire     404.5     542.7   3.45   2.46  \n",
       "            Rhode Island      498.8     613.5  16.93  11.55  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#COMBINE INTO ONE DATASET\n",
    "\n",
    "weather_df = df_combined_grouped.copy()\n",
    "disaster = disaster_group.copy()\n",
    "\n",
    "full_df = (disaster.join(weather_df, how = 'outer'))\n",
    "full_df.fillna(0, inplace = True)\n",
    "\n",
    "\n",
    "# get possible regions codes from dataset \n",
    "states_region_df = disaster_merge_df[['state', 'region']].drop_duplicates()\n",
    "region_codes = sorted(states_region_df.drop_duplicates()['region'].unique().tolist())\n",
    "\n",
    "# build a dictionary of region codes and state names \n",
    "state_region_dict = {}\n",
    "for i in region_codes:\n",
    "    states_list = states_region_df[states_region_df['region']==i]['state'].values\n",
    "    for j in states_list:\n",
    "        state_region_dict[j] = str(i)\n",
    "\n",
    "# adding region column back into the merged dataset\n",
    "full_df = full_df.reset_index()\n",
    "full_df['region'] = full_df['state'].map(state_region_dict)\n",
    "\n",
    "# showing the full dataframe\n",
    "full_df.groupby(['region','year','state']).sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns for weather data features and disaster type features \n",
    "weather_features = full_df.columns[-7:-1].values.tolist()\n",
    "disaster_features = full_df.columns[3:-7].values.tolist()\n",
    "other_features = ['year','month', 'state', 'region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56928ccdd5b34b8d9363ead66675e8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='features', options=('percipitation', 'max_temp', 'min_temp', 'avg_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AVG TEMP OF THE NATION OVER YEARS\n",
    "\n",
    "# plotting average temp by year to see the trend for global warming \n",
    "\n",
    "weather_df_by_year = full_df[other_features + weather_features].drop(['month', 'state','region'], axis=1).groupby('year').mean()\n",
    "\n",
    "# drop down menu allowing to switch features \n",
    "@interact (features=weather_df_by_year.columns)\n",
    "#@interact (features=avg_grouped_by_year.index)\n",
    "\n",
    "def plot_features(features):\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 7))\n",
    "    plt.title(str(features) + \" trends between 1953 - 2023 CONUS\")\n",
    "    plt.plot(weather_df_by_year.index, weather_df_by_year[features])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# list of states and features \\nlist_of_features = weather_df.columns\\nlist_of_states = weather_df.reset_index()[\\'state\\'].unique()\\n\\n# drop down menus of options \\nstate_selection = widgets.Dropdown(options=list_of_states, value=list_of_states[0], description=\\'States:\\', disabled=False)\\nfeatures_selection = widgets.Dropdown(options=list_of_features, value=list_of_features[0], description=\\'Features:\\', disabled=False)\\n\\n# fucntion to interatively draw graphs based on filters \\ndef draw_graph(selected_state, selected_feature):\\n    feature_data_per_state = weather_df.reset_index().drop(\\'month\\', axis=1)\\n    feature_data_per_state = feature_data_per_state[feature_data_per_state[\\'state\\'] == selected_state]\\n    feature_data_per_state = feature_data_per_state.drop(\\'state\\', axis=1).groupby(\\'year\\').mean()\\n\\n    plt.title(selected_feature + \" trend of between 1953 and 2023 in \" + selected_state)\\n    plt.plot(feature_data_per_state.index, feature_data_per_state[selected_feature])\\n\\ninteractive(draw_graph, selected_state = state_selection, selected_feature = features_selection)\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# list of states and features \n",
    "list_of_features = weather_df.columns\n",
    "list_of_states = weather_df.reset_index()['state'].unique()\n",
    "\n",
    "# drop down menus of options \n",
    "state_selection = widgets.Dropdown(options=list_of_states, value=list_of_states[0], description='States:', disabled=False)\n",
    "features_selection = widgets.Dropdown(options=list_of_features, value=list_of_features[0], description='Features:', disabled=False)\n",
    "\n",
    "# fucntion to interatively draw graphs based on filters \n",
    "def draw_graph(selected_state, selected_feature):\n",
    "    feature_data_per_state = weather_df.reset_index().drop('month', axis=1)\n",
    "    feature_data_per_state = feature_data_per_state[feature_data_per_state['state'] == selected_state]\n",
    "    feature_data_per_state = feature_data_per_state.drop('state', axis=1).groupby('year').mean()\n",
    "\n",
    "    plt.title(selected_feature + \" trend of between 1953 and 2023 in \" + selected_state)\n",
    "    plt.plot(feature_data_per_state.index, feature_data_per_state[selected_feature])\n",
    "\n",
    "interactive(draw_graph, selected_state = state_selection, selected_feature = features_selection)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4924e147164ed3b393b853a57cdf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='States:', options=('Alabama', 'Arizona', 'Arkansas', 'Cali…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list of states and features \n",
    "list_of_features = weather_df.columns\n",
    "list_of_states = weather_df.reset_index()['state'].unique()\n",
    "\n",
    "# drop down menus of options \n",
    "state_selection = widgets.Dropdown(options=list_of_states, value=list_of_states[0], description='States:', disabled=False)\n",
    "features_selection = widgets.Dropdown(options=list_of_features, value=list_of_features[0], description='Features:', disabled=False)\n",
    "\n",
    "# fucntion to interatively draw graphs based on filters \n",
    "def draw_graph(selected_state, selected_feature):\n",
    "    feature_data_per_state = weather_df.reset_index().drop('month', axis=1)\n",
    "    feature_data_per_state = feature_data_per_state[feature_data_per_state['state'] == selected_state]\n",
    "    feature_data_per_state = feature_data_per_state.drop('state', axis=1).groupby('year').mean()\n",
    "\n",
    "    disaster_df_slice = full_df[other_features + disater_features]\n",
    "    disaster_df_slice = disaster_df_slice.drop(['month', 'region'], axis=1)\n",
    "    disaster_total_occurance_df = disaster_df_slice.groupby(['year','state']).sum().T.sum()\n",
    "    disaster_total_occurance_df = disaster_total_occurance_df.reset_index()\n",
    "    disaster_total_occurance_df = disaster_total_occurance_df.rename(columns={0:'total_disaster_occurances'})\n",
    "    disaster_total_occurance_per_state = disaster_total_occurance_df[disaster_total_occurance_df['state'] == selected_state]\n",
    "\n",
    "    # set figure size \n",
    "    fig, ax1 = plt.subplots(figsize=(15, 7))\n",
    "    #ax1.set_xlabel('year')\n",
    "    ax1.set_ylabel(selected_feature)\n",
    "    plt.title(selected_feature + \" trend of between 1953 and 2023 in \" + selected_state)\n",
    "    # temperature line chart \n",
    "    plt.plot(feature_data_per_state.index, feature_data_per_state[selected_feature])\n",
    "    ax1.tick_params(axis='y')\n",
    "\n",
    "    # disaster occurance bar chart\n",
    "    ax2 = ax1.twinx()\n",
    "    plt.bar(disaster_total_occurance_per_state['year'], disaster_total_occurance_per_state['total_disaster_occurances'], color='orange', alpha=0.5)\n",
    "    ax2.set_ylabel('total occurances of disasters')\n",
    "    ax2.tick_params(axis='y')\n",
    "\n",
    "# fixing layout options \n",
    "widget = interactive(draw_graph, selected_state = state_selection, selected_feature = features_selection)\n",
    "menu = HBox(widget.children[:-1], layout = Layout(flex_flow='row wrap'))\n",
    "graph = output = widget.children[-1]\n",
    "display(VBox([menu, graph]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec87045c15c845898f8b9c54ad6d97c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='features', options=('percipitation', 'max_temp', 'min_temp', 'avg_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_disaster = full_df[other_features + disaster_features].drop(['month','state','region'], axis=1).groupby('year').sum().T.sum().reset_index().rename(columns={0:'disaster_occurance'})\n",
    "total_disaster= total_disaster.set_index('year')\n",
    "\n",
    "weather_df_avg_year = weather_df.reset_index().drop(['month','state'], axis=1).groupby('year').mean()\n",
    "\n",
    "joined_disaster_weather = total_disaster.join(weather_df_avg_year)\n",
    "\n",
    "\n",
    "features = widgets.Dropdown(options=weather_features, value=weather_features[0], description='Features:', disabled=False)\n",
    "\n",
    "@interact (features=weather_features)\n",
    "\n",
    "def plot_features(features):\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 7))\n",
    "    plt.title('relation between ' + str(features) + ' and natural disaster occurances')\n",
    "    a, b = np.polyfit(joined_disaster_weather['disaster_occurance'], joined_disaster_weather[features], 1)\n",
    "    plt.scatter(joined_disaster_weather[features], joined_disaster_weather['disaster_occurance'])\n",
    "    plt.plot( a*joined_disaster_weather+b, joined_disaster_weather)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked Bar Charts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38490f88dc946978f5455957e6a5be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='States:', options=('Alabama', 'Arizona', 'Arkansas', 'California',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_selection = widgets.Dropdown(options=list_of_states, value=list_of_states[0], description='States:', disabled=False)\n",
    "\n",
    "@interact (features=state_selection)\n",
    "\n",
    "def plot_features(features):\n",
    "    #fig, ax1 = plt.subplots(figsize=(9, 7))\n",
    "    #plt.title('relation between ' + str(features) + ' and natural disaster occurances')\n",
    "    #a, b = np.polyfit(joined_disaster_weather['disaster_occurance'], joined_disaster_weather[features], 1)\n",
    "    #plt.scatter(joined_disaster_weather[features], joined_disaster_weather['disaster_occurance'])\n",
    "    #plt.plot( a*joined_disaster_weather+b, joined_disaster_weather)\n",
    "\n",
    "    plot_df = full_df[other_features + disaster_features].drop(['month','region'], axis=1)\n",
    "    selected_df = plot_df[plot_df['state']==features].drop('state', axis=1).groupby('year').sum().reset_index()\n",
    "    selected_df.plot(x='year', kind='bar', stacked=True, figsize=(17,7))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1,0.6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
